<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Machine Learning Introduction | Guader's Blog</title><meta name="author" content="Guader"><meta name="copyright" content="Guader"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="How Models WorkWe’ll start with a model called Decision Tree. There are fancier models that give more accurate predictions. But desision trees are easy to understand, and they are the basic building b">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning Introduction">
<meta property="og:url" content="https://guaderxx.github.io/2023/06/30/ml-intro/index.html">
<meta property="og:site_name" content="Guader&#39;s Blog">
<meta property="og:description" content="How Models WorkWe’ll start with a model called Decision Tree. There are fancier models that give more accurate predictions. But desision trees are easy to understand, and they are the basic building b">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://guaderxx.github.io/img/avatar.png">
<meta property="article:published_time" content="2023-06-30T14:11:35.000Z">
<meta property="article:modified_time" content="2024-07-03T02:48:53.517Z">
<meta property="article:author" content="Guader">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="ML">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://guaderxx.github.io/img/avatar.png"><link rel="shortcut icon" href="../../../../img/fav.png"><link rel="canonical" href="https://guaderxx.github.io/2023/06/30/ml-intro/index.html"><link rel="preconnect" href="https://jsd.012700.xyz"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="stylesheet" href="../../../../css/index.css?v=4.13.0"><link rel="stylesheet" href="https://jsd.012700.xyz/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://jsd.012700.xyz/npm/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://jsd.012700.xyz/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-9ZF8XKZV7M"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-9ZF8XKZV7M');
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: {"limitDay":90,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":800},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Error',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"You have switched to Traditional Chinese","cht_to_chs":"You have switched to Simplified Chinese","day_to_night":"You have switched to Dark Mode","night_to_day":"You have switched to Light Mode","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"bottom-right"},
  infinitegrid: {
    js: 'https://jsd.012700.xyz/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Machine Learning Introduction',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-07-03 10:48:53'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="../img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="../archives/"><div class="headline">Articles</div><div class="length-num">34</div></a><a href="../tags/"><div class="headline">Tags</div><div class="length-num">16</div></a><a href="../categories/"><div class="headline">Categories</div><div class="length-num">28</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="../index.html"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="../archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="../tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="../categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="../link/"><i class="fa-fw fas fa-link"></i><span> Bookmark</span></a></div><div class="menus_item"><a class="site-page" href="../about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="../index.html" title="Guader's Blog"><span class="site-name">Guader's Blog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="../index.html"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="../archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="../tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="../categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="../link/"><i class="fa-fw fas fa-link"></i><span> Bookmark</span></a></div><div class="menus_item"><a class="site-page" href="../about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Machine Learning Introduction</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-06-30T14:11:35.000Z" title="Created 2023-06-30 22:11:35">2023-06-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-07-03T02:48:53.517Z" title="Updated 2024-07-03 10:48:53">2024-07-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="../../../../categories/Intro/">Intro</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word Count:</span><span class="word-count">2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading Time:</span><span>12mins</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Machine Learning Introduction"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><h2 id="How-Models-Work"><a href="#How-Models-Work" class="headerlink" title="How Models Work"></a>How Models Work</h2><p>We’ll start with a model called Decision Tree. There are fancier models that give more accurate predictions. But desision trees are easy to understand, and they are the basic building block for some of the best models in data science.</p>
<p>For simplicity, we’ll start with the simplest possible decision tree.</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">    Sample Decision Tree</span><br><span class="line">    </span><br><span class="line">        ----------------------------------------</span><br><span class="line">        |<span class="string"> Does house have more than 2 bedrooms </span>|</span><br><span class="line">        ----------------------------------------</span><br><span class="line">    / no                                       \ yes</span><br><span class="line">--------------------                 --------------------</span><br><span class="line">|<span class="string"> Predicted Price: </span>|<span class="string">                 </span>|<span class="string"> Predicted Price: </span>|</span><br><span class="line">|<span class="string"> $178000          </span>|<span class="string">                 </span>|<span class="string"> $188000          </span>|</span><br><span class="line">--------------------                 --------------------</span><br></pre></td></tr></table></figure>

<p>It divides houses into only two categories. The predicted price for any house under consideration is the historical average price of houses in the same category.</p>
<p>We use data to decide how to break the houses into two groups, and then again to determine the predicted price in each group. This step of capturing patterns from data is called <strong>fitting</strong> or <strong>training</strong> the model. The data used to <strong>fit</strong> the model is called the <strong>training data</strong>.</p>
<p>The details of how the model is fit(e.g. how to split up the data) is complex enough that we will save it for later. After the model has been fit, you can apply it to new data to <strong>predict</strong> prices of additional homes.</p>
<h3 id="Improving-the-Decision-Tree"><a href="#Improving-the-Decision-Tree" class="headerlink" title="Improving the Decision Tree"></a>Improving the Decision Tree</h3><p>Which of the following two decision trees is more likely to result from fitting the real estate training data?</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">          1st Decision Tree                              2nd Decision Tree</span><br><span class="line">        ------------------------                       ------------------------</span><br><span class="line">        |<span class="string"> Does house have more </span>|<span class="string">                       </span>|<span class="string"> Does house have more </span>|</span><br><span class="line">        |<span class="string"> than 2 bedrooms      </span>|<span class="string">                       </span>|<span class="string"> than 2 bedrooms      </span>|</span><br><span class="line">        ------------------------                       ------------------------</span><br><span class="line">     / no                     \ yes                    / no                    \ yes</span><br><span class="line">--------------------      --------------------    --------------------    --------------------</span><br><span class="line">|<span class="string"> Predicted Price: </span>|<span class="string">      </span>|<span class="string"> Predicted Price: </span>|<span class="string">    </span>|<span class="string"> Predicted Price: </span>|<span class="string">    </span>|<span class="string"> Predicted Price: </span>|</span><br><span class="line">|<span class="string"> $178000          </span>|<span class="string">      </span>|<span class="string"> $188000          </span>|<span class="string">    </span>|<span class="string"> $188000          </span>|<span class="string">    </span>|<span class="string"> %178000          </span>|</span><br><span class="line">--------------------      --------------------    --------------------    --------------------</span><br></pre></td></tr></table></figure>

<p>The descision tree on the left(Decision Tree 1) probably makes more sense. The biggest shortcoming of this model is that it doesn’t capture most factors affecting home price, like number of bathrooms, lot size, location, etc.</p>
<p>You can capture more factors using a tree that has more “splits”. These are called “deeper” trees. A decision tree that also considers the total size of each house’s lot might look like this:</p>
<figure class="highlight gherkin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">                           ----------------------------------------</span><br><span class="line">                           |<span class="string"> Does house have more than 2 bedrooms </span>|</span><br><span class="line">                           ----------------------------------------</span><br><span class="line">                           / no                                   \ yes</span><br><span class="line">       -----------------------------------------      ------------------------------------------</span><br><span class="line">       |<span class="string"> Lot size larger than 8500 square feet </span>|<span class="string">      </span>|<span class="string"> Lot size larger than 11500 square feet </span>|</span><br><span class="line">       -----------------------------------------      ------------------------------------------</span><br><span class="line">           / no                      \ yes                      / no               \ yes</span><br><span class="line">--------------------        --------------------      --------------------        --------------------</span><br><span class="line">|<span class="string"> Predicted Price: </span>|<span class="string">        </span>|<span class="string"> Predicted Price: </span>|<span class="string">      </span>|<span class="string"> Predicted Price: </span>|<span class="string">        </span>|<span class="string"> Predicted Price: </span>|</span><br><span class="line">|<span class="string"> $146000          </span>|<span class="string">        </span>|<span class="string"> $188000          </span>|<span class="string">      </span>|<span class="string"> $170000          </span>|<span class="string">        </span>|<span class="string"> $233000          </span>|</span><br><span class="line">--------------------        --------------------      --------------------        --------------------</span><br></pre></td></tr></table></figure>


<h2 id="Your-First-Machine-Learning-Model"><a href="#Your-First-Machine-Learning-Model" class="headerlink" title="Your First Machine Learning Model"></a>Your First Machine Learning Model</h2><h3 id="Selecting-Data-for-Modeling"><a href="#Selecting-Data-for-Modeling" class="headerlink" title="Selecting Data for Modeling"></a>Selecting Data for Modeling</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(<span class="string">&quot;file_path&quot;</span>, index_col=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get all column names</span></span><br><span class="line">data.columns</span><br><span class="line"></span><br><span class="line"><span class="comment"># drop missing values (think of na as &quot;not available&quot;)</span></span><br><span class="line">data = data.dropna(axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>


<h3 id="Selecting-The-Prediction-Target"><a href="#Selecting-The-Prediction-Target" class="headerlink" title="Selecting The Prediction Target"></a>Selecting The Prediction Target</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = data.Price</span><br></pre></td></tr></table></figure>


<h3 id="Choosing-“Features”"><a href="#Choosing-“Features”" class="headerlink" title="Choosing “Features”"></a>Choosing “Features”</h3><p>The columns that are inputted into our model (and later used to make predictions) are called “features”. In our case, those would be the columns used to determine the home price.</p>
<p>For now, we’ll build a model with only a few features. Later on you’ll see how to iterate and compare models built with different features.</p>
<p>We select multiple features by providing a list of column names inside brackets. Each item in that list should be a string(with quotes).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">features = [<span class="string">&quot;Rooms&quot;</span>, <span class="string">&quot;Bathroom&quot;</span>, <span class="string">&quot;Landsize&quot;</span>, <span class="string">&quot;Lattitude&quot;</span>, <span class="string">&quot;Longtitude&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># by conversion, this data is called X</span></span><br><span class="line">X = data[features]</span><br></pre></td></tr></table></figure>


<h3 id="Building-Your-Model"><a href="#Building-Your-Model" class="headerlink" title="Building Your Model"></a>Building Your Model</h3><p>You will use the <strong>scikit-learn</strong> library to create your models. When coding, this library is written as <strong>sklearn</strong>, as you will see in the sample code. Scikit-learn is easily the most popular library for modeling the types of data typically stored in DataFrames.</p>
<p>The steps to building and using a model are:</p>
<ul>
<li><strong>Define</strong>: What type of model will it be? A Decision Tree? Some other type of model? Some other parameters of the model type are specified too.</li>
<li><strong>Fit</strong>:  Capture patterns from provided data. This is the heart of modeling.</li>
<li><strong>Predict</strong>: Just what it sounds like.</li>
<li><strong>Evaluate</strong>: Determine how accurate the model’s predictions are.</li>
</ul>
<p>Here is an example of defining a decision tree model with scikit-learn and fitting it with the features and target variable.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define model. Specify a number for random_state to ensure same results each run</span></span><br><span class="line">model = DecisionTreeRegressor(random_state=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit model</span></span><br><span class="line">model.fix(X, y)</span><br></pre></td></tr></table></figure>

<p>We now have a fitted model that we can use to make predictions.</p>
<p>In practise, you’ll want to make predictions for new houses coming on the market rather than the houses we already have prices for. But we’ll make predictions for the first few rows of the training data to see how the predict function works.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Making predictions for the following 5 houses:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(X.head())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;The predictions are: &quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(model.predict(X.head()))</span><br></pre></td></tr></table></figure>


<h2 id="Model-Validation"><a href="#Model-Validation" class="headerlink" title="Model Validation"></a>Model Validation</h2><p>Measure the performance of your model, so you can test and compare alternatives.</p>
<h3 id="What-is-Model-Validation"><a href="#What-is-Model-Validation" class="headerlink" title="What is Model Validation"></a>What is Model Validation</h3><p>In most (though not all) applications, the relevant measure of model quality is predictive accuracy. In other words, will the model’s predictions be close to what actually happens.</p>
<p>Many people make a huge mistake when measuring predictive accuracy. They make predictions with their training data and compare those predictions to the target values in the training data. You’ll see the problem with this approach and how to solve it in a moment, but let’s think about how we’d do this first.</p>
<p>There are many metrics for summarizing model quality, but we’ll start with one called <strong>Mean Absolute Error</strong> MAE. Let’s break down this metric starting with the last word, error.</p>
<p>The prediction error for each house is:</p>
<p><code>error = actual - predicted</code></p>
<p>So, if a house cose $150,000 and you predicted it would cost $100,000 the error is $50,000.</p>
<p>With the MAE metric, we take the absolute value of each error. This converts each error to a positive number. We than take the average of those absolute errors. This is our measure of quality. In plain English, it can be said as:</p>
<blockquote>
<p>On average, our predictions are off by about X.</p>
</blockquote>
<p>To calculate MAE, we first need a model.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(<span class="string">&quot;file_path&quot;</span>)</span><br><span class="line"><span class="comment"># filter rows with missing values</span></span><br><span class="line">filter_data = data.dropna(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">feature_names = [<span class="string">&quot;col1&quot;</span>, <span class="string">&quot;col2&quot;</span>, <span class="string">&quot;col3&quot;</span>, <span class="string">&quot;col4&quot;</span>]</span><br><span class="line">X = filter_data[feature_names]</span><br><span class="line">y = filter_data[<span class="string">&quot;col5&quot;</span>]</span><br><span class="line"></span><br><span class="line">model = DecisionTreeRegressor(random_state=<span class="number">1</span>)</span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># model.predict(X.head())</span></span><br></pre></td></tr></table></figure>

<p>Once we have a model, here is how we calculate the mean absolute error:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error</span><br><span class="line"></span><br><span class="line">predicted_value = model.predict(X)</span><br><span class="line">mean_absolute_error(y, predicted_value)</span><br></pre></td></tr></table></figure>


<h3 id="The-Problem-with-“In-Sample”-Scores"><a href="#The-Problem-with-“In-Sample”-Scores" class="headerlink" title="The Problem with “In-Sample” Scores"></a>The Problem with “In-Sample” Scores</h3><p>The measure we just computed can be called an “In-Sample” score. We used a single “sample” of houses for both building the model and evaluating it. Here’s why this is bad.</p>
<p>The most straightforward way to do this is to exclude some data from the model-building process, and then use those to test the model’s accuracy on data it hasn’t seen before. This data is called <strong>validation data</strong>.</p>
<p>The scikit-learn library has a function <code>train_test_split</code> to break up the data into two pieces. We’ll use some of that data as training data to fit the model, and we’ll use the other data as <em>validation data</em> to calculate <code>mean_absolute_error</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># define model</span></span><br><span class="line">model = DecisionTreeRegressor()</span><br><span class="line"><span class="comment"># fit model</span></span><br><span class="line">model.fit(train_X, train_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get predicted value on validation data</span></span><br><span class="line">val_predictions = model.predict(val_X)</span><br><span class="line"></span><br><span class="line">mean_absolute_error(val_y, val_predictions)</span><br></pre></td></tr></table></figure>


<h2 id="Underfitting-and-Overfitting"><a href="#Underfitting-and-Overfitting" class="headerlink" title="Underfitting and Overfitting"></a>Underfitting and Overfitting</h2><p>Fine-tune your model for better performance.</p>
<h3 id="Experimenting-With-Different-Models"><a href="#Experimenting-With-Different-Models" class="headerlink" title="Experimenting With Different Models"></a>Experimenting With Different Models</h3><p>You can see in scikit-learn’s <a target="_blank" rel="noopener" href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html">documentation</a> that the decision tree model has many options (more than you’ll want or need for a long time). The most important options determine the tree’s depth. Recall from the first lesson in this course that a tree’s depth is a measure of how many splits it makes before coming to a prediction.</p>
<p>Leaves with very few houses will make predictions that are quite close to those homes’ actual values, but they may make very unreliable predictions for new data (because each prediction is based on only a few houses).</p>
<p>This is a phenomenon called <strong>overfitting</strong>, where a model matches the training data almost perfectly, but does poorly in validation data and other new data. On the flip side, if we make our tree very shallow, it doesn’t divide up the houses into very distinct groups.</p>
<p>At an extreme, if a tree divides houses into only 2 or 4, each group still has a wide variety of houses. Resulting predictions may be far off for most houses, even in the training data (and it will be bad in validation too for the same reason). When a model fails to capture important distinctions and patterns in the data, so it performs poorly even in training data, that is called <strong>underfitting</strong>.</p>
<p><strong>Mean Absolute Error</strong></p>
<p><img src="https://storage.googleapis.com/kaggle-media/learn/images/AXSEOfI.png"></p>
<h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><p>There are a few alternatives for controlling the tree depth, and many allow for some routes through the tree to have greater depth than other routes. But the max_leaf_nodes argument provides a very sensible way to control overfitting vs underfitting. The more leaves we allow the model to make, the more we move from the underfitting area in the above graph to the overfitting area.</p>
<p>We can use a utility function to help compare MAE scores from different values for max_leaf_nodes:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_mae</span>(<span class="params">max_leaf_nodes, train_X, val_X, train_y, val_y</span>):</span><br><span class="line">    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=<span class="number">0</span>)</span><br><span class="line">    model.fit(train_X, train_y)</span><br><span class="line">    val_prediction = model.predict(val_X)</span><br><span class="line">    mae = mean_absolute_error(val_y, val_prediction)</span><br><span class="line">    <span class="keyword">return</span> mae</span><br></pre></td></tr></table></figure>

<p>The data is loaded into <code>train_X, val_X, train_y, val_y</code> using the code you’ve already seen (and which you’ve already written).</p>
<p>We can use a for-loop to compare the accuracy of models built with different values for <code>max_leaf_nodes</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># compare MAE with different max_leaf_nodes</span></span><br><span class="line"><span class="keyword">for</span> max_leaf_node <span class="keyword">in</span> [<span class="number">5</span>, <span class="number">50</span>, <span class="number">500</span>, <span class="number">5000</span>]:</span><br><span class="line">    my_mae = get_mae(max_leaf_node, train_X, val_X, train_y, val_y)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Max Leaf Node: <span class="subst">&#123;max_leaf_node&#125;</span>\t\t Mean Absolute Error: <span class="subst">&#123;my_mae&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>Of the options listed, 500 is the optimal number of leaves.</p>
<h2 id="Random-Forests"><a href="#Random-Forests" class="headerlink" title="Random Forests"></a>Random Forests</h2><p>Using a more sophisticated machine learning algorithm.</p>
<h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>Decision Tree leaves you with a difficult decision. A deep tree with lots of leaves will overfit because each prediction is coming from historical data from only the few houses at its leaf. But a shallow tree with few leaves will perform poorly because it fails to capture as many distinctions in the raw data. </p>
<p>Even today’s most sophisticated modeling techniques face this tension between underfitting and overfitting. But, many models have clever ideas that can lead to better performance. We’ll look at the random forest as an example.</p>
<h3 id="Example-1"><a href="#Example-1" class="headerlink" title="Example"></a>Example</h3><p>You’ve already seen the code to load the data a few times. At the end of data-loading, we have the following variables:</p>
<ul>
<li>train_X</li>
<li>val_X</li>
<li>train_y</li>
<li>val_y</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">file_path = <span class="string">&quot;file_path&quot;</span></span><br><span class="line">data = pd.read_csv(file_path)</span><br><span class="line">filtered_data = data.dropna(axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">y = filtered_data[<span class="string">&quot;price&quot;</span>]</span><br><span class="line">feature_name = [<span class="string">&quot;col_1&quot;</span>, <span class="string">&quot;col_2&quot;</span>, <span class="string">&quot;col_3&quot;</span>, <span class="string">&quot;col_4&quot;</span>]</span><br><span class="line">X = filtered_data[feature_name]</span><br><span class="line"></span><br><span class="line">train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>We build a random forest model similarly to how we built a decision tree in scikit-learn - this time using the <code>RandomForestRegressor</code> class instead of <code>DecisionTreeRegressor</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error</span><br><span class="line"></span><br><span class="line">model = RandomForestRegressor(random_state=<span class="number">1</span>)</span><br><span class="line">model.fit(train_X, train_y)</span><br><span class="line"></span><br><span class="line">val_preds = model.predict(val_X)</span><br><span class="line">mae = mean_absolute_error(val_y, val_preds)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mae)</span><br></pre></td></tr></table></figure>

<h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>There is likely room for further improvement, but this is a big improvement over the best decision tree error of 250,000. There are parameters which allow you to change the performance of the Random Forest much as we changed the maximum depth of the single decision tree. But one of the best features of Random Forest models is that they generally work reasonably even without this tuning.</p>
<h2 id="Conclusion-1"><a href="#Conclusion-1" class="headerlink" title="Conclusion"></a>Conclusion</h2><ol>
<li>Import Data</li>
<li>Select columns to predict and feature columns</li>
<li>Split Data into training data and validation data</li>
<li>Define the model</li>
<li>Fit model</li>
<li>Use metrics to summarize model quality</li>
<li>Choose the best quality model and training all of the data</li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://github.com/Guaderxx">Guader</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://guaderxx.github.io/2023/06/30/ml-intro/">https://guaderxx.github.io/2023/06/30/ml-intro/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="../../../../tags/Python/">Python</a><a class="post-meta__tags" href="../../../../tags/ML/">ML</a></div><div class="post_share"><div class="addtoany"><div class="a2a_kit a2a_kit_size_32 a2a_default_style"><a class="a2a_button_facebook"></a><a class="a2a_button_twitter"></a><a class="a2a_button_wechat"></a><a class="a2a_button_facebook_messenger"></a><a class="a2a_button_email"></a><a class="a2a_button_copy_link"></a><a class="a2a_dd" target="_blank" rel="noopener" href="https://www.addtoany.com/share"></a></div></div><script async="async" src="https://static.addtoany.com/menu/page.js"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="../../25/pandas/" title="pandas"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Previous</div><div class="prev_info">pandas</div></div></a></div><div class="next-post pull-right"><a href="../../../08/12/the-past-month/" title="the-past-month"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next</div><div class="next_info">the-past-month</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="../../25/pandas/" title="pandas"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2024-07-03</div><div class="title">pandas</div></div></a></div></div></div><hr class="custom-hr"/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div id="giscus-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#How-Models-Work"><span class="toc-number">1.</span> <span class="toc-text">How Models Work</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Improving-the-Decision-Tree"><span class="toc-number">1.1.</span> <span class="toc-text">Improving the Decision Tree</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Your-First-Machine-Learning-Model"><span class="toc-number">2.</span> <span class="toc-text">Your First Machine Learning Model</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Selecting-Data-for-Modeling"><span class="toc-number">2.1.</span> <span class="toc-text">Selecting Data for Modeling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Selecting-The-Prediction-Target"><span class="toc-number">2.2.</span> <span class="toc-text">Selecting The Prediction Target</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Choosing-%E2%80%9CFeatures%E2%80%9D"><span class="toc-number">2.3.</span> <span class="toc-text">Choosing “Features”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Building-Your-Model"><span class="toc-number">2.4.</span> <span class="toc-text">Building Your Model</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Model-Validation"><span class="toc-number">3.</span> <span class="toc-text">Model Validation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#What-is-Model-Validation"><span class="toc-number">3.1.</span> <span class="toc-text">What is Model Validation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#The-Problem-with-%E2%80%9CIn-Sample%E2%80%9D-Scores"><span class="toc-number">3.2.</span> <span class="toc-text">The Problem with “In-Sample” Scores</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Underfitting-and-Overfitting"><span class="toc-number">4.</span> <span class="toc-text">Underfitting and Overfitting</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Experimenting-With-Different-Models"><span class="toc-number">4.1.</span> <span class="toc-text">Experimenting With Different Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Example"><span class="toc-number">4.2.</span> <span class="toc-text">Example</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Random-Forests"><span class="toc-number">5.</span> <span class="toc-text">Random Forests</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Introduction"><span class="toc-number">5.1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Example-1"><span class="toc-number">5.2.</span> <span class="toc-text">Example</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Conclusion"><span class="toc-number">5.3.</span> <span class="toc-text">Conclusion</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Conclusion-1"><span class="toc-number">6.</span> <span class="toc-text">Conclusion</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2024 By Guader</div><div class="footer_custom_text">The best time to plant a tree was twenty years ago. The second best time is now :)</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light And Dark Mode"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="https://jsd.012700.xyz/npm/hexo-theme-butterfly@4.13.0/source/js/utils.min.js"></script><script src="https://jsd.012700.xyz/npm/hexo-theme-butterfly@4.13.0/source/js/main.min.js"></script><script src="https://jsd.012700.xyz/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script src="https://jsd.012700.xyz/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://jsd.012700.xyz/npm/node-snackbar@0.1.16/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://jsd.012700.xyz/npm/pangu@4.0.7/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>(()=>{
  const getGiscusTheme = theme => {
    return theme === 'dark' ? 'dark' : 'light'
  }

  const loadGiscus = () => {
    const config = Object.assign({
      src: 'https://giscus.app/client.js',
      'data-repo': 'Guaderxx/Guaderxx.github.io',
      'data-repo-id': 'R_kgDOJxPT0Q',
      'data-category-id': 'DIC_kwDOJxPT0c4CXUYG',
      'data-mapping': 'pathname',
      'data-theme': getGiscusTheme(document.documentElement.getAttribute('data-theme')),
      'data-reactions-enabled': '1',
      crossorigin: 'anonymous',
      async: true
    },null)

    const ele = document.createElement('script')
    for (let key in config) {
      ele.setAttribute(key, config[key])
    }
    document.getElementById('giscus-wrap').appendChild(ele)
  }

  const changeGiscusTheme = theme => {
    const sendMessage = message => {
      const iframe = document.querySelector('iframe.giscus-frame')
      if (!iframe) return
      iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app')
    }

    sendMessage({
      setConfig: {
        theme: getGiscusTheme(theme)
      }
    });
  }

  btf.addGlobalFn('themeChange', changeGiscusTheme, 'giscus')

  if ('Giscus' === 'Giscus' || !false) {
    if (false) btf.loadComment(document.getElementById('giscus-wrap'), loadGiscus)
    else loadGiscus()
  } else {
    window.loadOtherComment= loadGiscus
  }
})()</script></div><script defer="defer" id="ribbon" src="https://jsd.012700.xyz/npm/butterfly-extsrc@1.1.3/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script><script async data-pjax src="https://jsd.012700.xyz/npm/busuanzi@2.3.0/bsz.pure.mini.min.js"></script></div></body></html>